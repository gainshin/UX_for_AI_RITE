# 摘要：讓使用者相信 AI 結果符合倫理、準確且值得信賴。包括警示、同意、資料所有權、揭露、足跡、無痕模式、浮水印等。

## Caveat (警示)

### 頁面整體介紹

「Caveat（警示）」或稱「免責聲明」，用於提醒使用者 AI 系統可能出錯、不完整或有偏見。儘管此模式無處不在，但其在培養使用者批判性思維方面的有效性仍有待觀察。警示常見於：
- **聊天機器人**：輸入框下方或每個輸出上方。
- **文件助理**：生成章節的頂部。
- **API 回應**：中繼資料中的警告標頭。
- **語音助理**：結果前後的語音宣告。

### 局限性

警示對非技術使用者有幫助，但 AI 模型複雜且快速發展，單純的免責聲明不足以引導使用者。此外，由於其普遍性，使用者可能已對其視而不見。應結合 `Wayfinders`（引導）、`Citations`（引用）等模式，協助使用者獲得更好的結果，而非僅依賴免責聲明。

### 設計建議

- **可見且不干擾**：將警示放在使用者決策的關鍵點，而非隱藏在頁尾。
- **語言清晰簡潔**：使用簡單易懂的語言，並連結到更技術性的文件。
- **與情境相關**：針對性的提醒（如「請核對日期準確性」）比籠統的警告更有效。
- **作為輔助系統的一部分**：警示應與 `Wayfinders`、`References`、`Footprints` 等模式結合，協助使用者預防或從錯誤中恢復。
- **假設會被忽略**：不要認為警示足以阻止有害行為，需主動評估模型以防止幻覺、偏見等問題。

### 相關模式（Related Patterns）

- **`Citations`**：提供引用，讓使用者能驗證模型的邏輯與參考資料，避免盲目信任。
- **`Attachments`**：方便使用者提供更豐富、可靠的上下文，減少模型產生幻覺的可能性。

### 案例示例（Pattern Examples）

- **`Adobe PDF`**：提供連結以獲取更多資訊。
- **`Character.ai`**：試圖警告使用者不要過於相信 AI。
- **`ChatGPT`**：最初的警示，後續更新為更直接的「檢查重要資訊」。
- **`Claude`**：在警示中保持其友善的語氣。
- **`Figjam`**：警示清晰直接。
- **`Github Copilot`**：在聊天視窗內分享直接的警示。
- **`NotionAI`**：提供文件連結。

---

## Consent (同意)

### 頁面整體介紹

在與 AI 共享資料前，明確取得「Consent（同意）」有助於建立信任、提供責任保護並支持合乎道德的使用者體驗。尤其當模型可以利用我們的聲音和對話內容進行訓練時，人們對隱私的擔憂更為強烈。同意主要涉及三個領域：
- **個人資料**：對話是否可被記錄或分析，特別是當資料可能用於模型訓練時。
- **組織資料**：避免與第三方 AI 共享專有資訊。
- **他人資料**：記錄或訓練不屬於主要使用者的聲音、肖像或文字。

### 主要形式與變體

- **`Opt-in disclosure`（選擇加入）**：使用者在產品記錄或使用資料前主動同意。
- **`Silent by default`（預設靜默）**：產品在不通知他人的情況下捕捉資料。
- **`Post-hoc alerts`（事後提醒）**：產品記錄後再通知參與者。
- **`Consent for training`（訓練同意）**：明確請求使用聲音、肖像或使用者內容進行模型微調。

### 特定情境

- **`開發個人肖像的同意`**：複製名人聲音或形象需取得同意，如 `ElevenLabs` 徵求名人遺產的許可。
- **`硬體設備的同意`**：如 `Meta` 的 AI 眼鏡，其無錄影提示的設計引發了騷擾他人的爭議。設計應考慮更道德的方法。

### 設計建議

- **明確且可見**：同意不應依賴隱晦的文字或圖示，錄製或分析時應有清晰、持續的指示。
- **預設選擇加入**：沉默不代表同意，應在捕捉資料前請求許可。
- **區分同意類型**：記錄、模型訓練、共享應為獨立選項。
- **隨時可撤回**：提供簡單的方式停止資料收集或刪除已捕捉的資料。
- **使用多模態指示**：在無螢幕介面中，透過燈光、聲音或震動提示。
- **為共享環境設計**：在會議或群聊中，所有參與者都應被通知。
- **闡明下游用途**：若資料用於訓練模型，需揭露其流向及匿名化方式。
- **平衡組織與個人**：企業管理員可預設政策，但個人使用者仍需收到明確確認。
- **為尊嚴而非僅為合規設計**：超越法律形式，明確的徵求同意傳達了尊重。

### 相關模式（Related Patterns）

- **`Disclosure`**：告知他人 AI 功能正在收集資料。
- **`Caveat`**：傳達共享資料的風險。
- **`Data ownership`**：透過設定和權限，讓使用者負責他們共享的內容。

### 案例示例（Pattern Examples）

- **`Fireflies`**：允許使用者從確認郵件中選擇不被錄製。
- **`Gong`**：提前發送通知，讓參與者可選擇不被錄製。
- **`Granola`**：提供可選設定，讓他人知道正在使用轉錄功能。
- **`Limitless pendant`**：曾有選項只錄製已口頭同意者的聲音。
- **`Notion`**：開始轉錄時顯示滾動字幕，提醒使用者需明確請求同意。

---

## Data ownership (資料所有權)

### 頁面整體介紹

「Data ownership（資料所有權）」處理的是使用者與 AI 互動時，資料歸屬權的界線。許多 AI 公司實施了資料保留模式，通常是在設定中提供一個開關，讓使用者決定其資料是否可用於模型訓練。

### 互動原則與差異

- **`Opt-in vs. opt-out`（選擇加入 vs. 退出）**：多數產品（`ChatGPT`、`Substack`）預設為開啟，即同意分享資料。值得注意的是，`Figma` 預設為關閉，顯示了客戶優先的趨勢。
- **`Paid vs. Unpaid`（付費 vs. 未付費）**：選擇退出的權利通常僅限於付費方案。免費使用者以其資料作為代價。
- **`Enterprise vs. consumer`（企業 vs. 消費者）**：企業級帳戶的設定通常由管理員控制，以確保公司安全政策的執行。
- **`Full privacy`（完全隱私）**：部分公司（如 `Limitless.ai`）完全不使用使用者資料進行模型訓練，並在其隱私政策中明確聲明。

### 設計建議

- **預設為隱私**：使用者不應對其資料被用於訓練感到驚訝。預設應限制訓練，但允許使用者隨時選擇加入。
- **區分訓練與保留**：將模型訓練的權限與出於客服目的保留資料的權限分開。
- **在 UI 中說明預設值**：在設定介面提供足夠的細節，讓使用者了解他們選擇的後果，而非僅在政策文件中說明。
- **呈現雙方觀點**：解釋資料如何被使用，以及允許資料用於訓練可能如何改善其體驗，讓使用者在資訊充足的情況下做決定。

### 相關模式（Related Patterns）

- **`Consent`**：當使用者可能共享他人資料時（如通話錄音），確保所有人都同意。
- **`Memory`**：將資料所有權與個人化快取（如使用者記憶）分開。
- **`Disclosure`**：揭露 AI 的存在及資料收集行為。

### 案例示例（Pattern Examples）

- **`ChatGPT`**：以利他主義的口吻鼓勵使用者分享資料。
- **`Claude`**：在使用者回報意見時，揭露資料所有權相關資訊。
- **`Gemini`**：在初次請求資料共享時，解釋其原因。
- **`Github`**：在其資料所有權設定中提供了比多數產品更詳細的資訊。
- **`Notion`**：以一個顯眼的彈出視窗請求使用者選擇加入資料共享。
- **`Perplexity`**：採用預設選擇加入的標準模式。
- **`Substack`**：在其設定面板中加入警示。

---

## Disclosure (揭露)

### 頁面整體介紹

AI 存在信任問題。「Disclosure（揭露）」模式透過明確標示 AI 互動與內容，幫助使用者區分 AI 生成的結果，從而建立信任。當使用者在資訊充分的情況下選擇體驗，他們更可能暫時放下疑慮，讓產品有機會展現價值。

### 設計建議

- **清晰標示**：在 AI 生成的內容或由 AI 驅動的互動旁，使用清晰的標籤或圖示。
- **情境化揭露**：在使用者需要做出判斷的關鍵時刻（如分享、發布前）提供揭露資訊。
- **保持一致性**：在整個產品中，對 AI 的標示應使用一致的視覺語言和術語。
- **教育使用者**：不僅僅是標示，還應提供連結或簡短說明，解釋 AI 在其中的作用。

### 相關模式（Related Patterns）

- **`Consent`**：在收集資料或進行 AI 互動前，揭露其目的與方式是取得同意的前提。
- **`Watermark`**：作為一種特定的揭露形式，浮水印直接嵌入到 AI 生成的內容中。
- **`Caveat`**：揭露 AI 存在的同時，也應警示其潛在的局限性。

### 案例示例（Pattern Examples）

- **`Tactiq`**：在會議記錄中揭露 AI 轉錄的存在。
- **`TikTok`**：採用內容憑證標準，自動標示 AI 創作者或內容。
- **`Zapier`**：允許使用者在其工作流程步驟中加入 AI 揭露。
- **`Zoom`**：當 AI 進入會議時，以一個大型浮層通知所有參與者。

---

## 案例研究：社交平台的信任建立機制與暗黑模式風險

**主要結論：**社交平台為吸引用戶持續互動，往往在「信任建立」機制上表現出表面合規、實質操控，真正保護使用者權益需從主動透明、對等控制和演算法主權三個層面深度設計。

### 1. 警示（Caveat）機制

**Facebook** 在隱私設定頁面引入「提醒標語」告知使用者資料可能用於廣告投放。但此類警示通常：
- **位置隱蔽**：多數顯示在設定子選單底部，不在主要操作流程中。[1]
- **語言模糊**：採用「可能」「通常」等不具約束力措辞。

**暗黑模式風險**  
- **責任外包**：將合規責任轉移到使用者閱讀警示上。  
- **疲勞效應**：頻繁或冗長的警示讓使用者視而不見。

### 2. 同意（Consent）流程

**Instagram** 在首次使用相機或麥克風功能時彈出許可授權對話方塊。其設計問題在於：
- **預設同意優勢**：允許「僅在使用時詢問」設定下，默認授權存取，且未明顯區分持續授權與單次使用。  
- **摩擦不對稱**：撤回權限需多步驟操作且分散在系統與應用內不同頁面。

**暗黑模式風險**  
- **社會責任綁架**：描述拒絕許可會影響「社群貢獻」，誘導使用者默認同意。  
- **選擇架構偏見**：將必要權限與可選權限合併呈現，降低使用者辨識度。

### 3. 資料所有權（Data Ownership）

**LinkedIn** 收集使用者職涯歷程與連結網絡，並在平台協議中聲明「分析成果歸 LinkedIn 所有」。  
- **分層權益**：對免費用戶無償取得資料使用權；對付費企業用戶提供更高級的分析工具，借此強化商業價值錨定。  

**暗黑模式風險**  
- **經濟脅迫**：將資料控制權與付費方案綁定，形成「付費者主導」的使用者階層。  
- **隱性資料流轉**：條款中允許將使用者資料提供給全資及投資關係企業，卻未在界面明示。

### 4. 揭露（Disclosure）與足跡（Footprints）

**TikTok** 在「活動記錄」中僅顯示過去 7 日的互動摘要，未完整揭露演算法推薦與瀏覽歷史。  
- **初次揭露**：使用者創建帳號時顯示「我們如何推薦影片」的簡易流程圖，但不再主動提示。  
- **持續足跡**：用戶無法批量下載或清除推薦參考的行為數據。

**暗黑模式風險**  
- **漸進式隱藏**：先以透明度建立信任，後續逐漸減少揭露頻率。  
- **全面記錄**：平台保留詳細觀看、點讚、停留時間等隱私敏感足跡，並可能與廣告商共享。

### 5. 無痕模式（Incognito）與浮水印（Watermark）

#### 無痕模式  
**Snapchat** 提供「私聊聊天」可設定「閱後即焚」，但該模式下仍會在伺服器緩存消息數小時以便審查。  
- **虛假無痕**：聲稱聊天不留痕，但未完全刪除伺服器備份。  
- **功能懲罰**：私聊功能較公開對話少可用濾鏡與特效，誘導放棄隱私模式。

#### 浮水印  
**Pinterest** 在 AI 生成的圖像工具中嵌入微小浮水印，但因同色系且尺寸極小，使用者難以察覺。  
- **被動浮水印**：不會自動提示存在浮水印，僅作合規備援。  
- **易被移除**：缺乏語義層級防護，技術駭客可輕易去除水印。

**暗黑模式風險**  
- **信任劇場**：提供象徵性功能以掩蓋真實資料或內容使用方式。  
- **隱性懲罰**：將隱私或標示功能與平台完整體驗掛鉤，降低使用者採用意願。

### 總結與防範建議

要避免社交平台「信任建立」功能淪為精緻的暗黑模式，應從以下三方面著手：

1. **主動透明**：在關鍵操作前後都要以易懂語言揭露使用者資料流向與演算法決策邏輯。  
2. **對等控制**：所有涉及資料權限的同意與撤回應具有相同易用性，且不綁定商業利益。  
3. **演算法主權**：提供使用者可解釋的推薦原因與全面的足跡管理，並允許真正「一鍵清除」所有歷史記錄。  

透過這些原則，使用者才能在社交平台上真正保持監督、自主與尊嚴，而不被偽裝的「信任」所蒙蔽。

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/85298212/8dd5df3c-9cd8-4f51-9486-804fff6582e3/Trust-builders.md)